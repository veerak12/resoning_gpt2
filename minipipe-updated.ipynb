{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11861310,"sourceType":"datasetVersion","datasetId":7453386}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% Install necessary libraries\n!pip install transformers accelerate trl peft bitsandbytes sympy matplotlib numpy scikit-learn datasets -q\n!pip install unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:23:30.274020Z","iopub.execute_input":"2025-05-18T17:23:30.274265Z","iopub.status.idle":"2025-05-18T17:28:29.992937Z","shell.execute_reply.started":"2025-05-18T17:23:30.274245Z","shell.execute_reply":"2025-05-18T17:28:29.992224Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting unsloth\n  Downloading unsloth-2025.5.6-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.5.7 (from unsloth)\n  Downloading unsloth_zoo-2025.5.7-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.5)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\nCollecting tyro (from unsloth)\n  Downloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers!=4.47.0,==4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\nRequirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.31.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (3.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.5.7->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.7->unsloth) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.5.7->unsloth)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.11.18)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth) (2025.4.26)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth) (1.17.0)\nDownloading unsloth-2025.5.6-py3-none-any.whl (265 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.6/265.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.5.7-py3-none-any.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.20-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, trl, xformers, unsloth_zoo, torchvision, unsloth\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: trl\n    Found existing installation: trl 0.17.0\n    Uninstalling trl-0.17.0:\n      Successfully uninstalled trl-0.17.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cut_cross_entropy-25.1.1 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.20 unsloth-2025.5.6 unsloth_zoo-2025.5.7 xformers-0.0.30\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# %% Imports\nimport json\nimport torch\nimport sympy as sp\nimport os\nimport matplotlib\nmatplotlib.use('Agg')  # For headless environments\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport shutil\nimport glob\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    GenerationConfig,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding\n)\nfrom unsloth import FastLanguageModel\nfrom trl import PPOTrainer, PPOConfig\nfrom datasets import Dataset\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:28:33.716001Z","iopub.execute_input":"2025-05-18T17:28:33.716793Z","iopub.status.idle":"2025-05-18T17:29:16.581219Z","shell.execute_reply.started":"2025-05-18T17:28:33.716761Z","shell.execute_reply":"2025-05-18T17:29:16.580631Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 17:28:47.533291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747589327.964289      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747589328.080580      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_35/2023442122.py:24: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n\nPlease restructure your imports with 'import unsloth' at the top of your file.\n  from unsloth import FastLanguageModel\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# folder_to_delete = \"/kaggle/working/sft_model\"\n# if os.path.exists(folder_to_delete):\n#     shutil.rmtree(folder_to_delete)\n#     print(f\"Successfully deleted the folder: {folder_to_delete}\")\n# else:\n#     print(f\"The folder does not exist: {folder_to_delete}\")\n\n# %% Define directories\nbase_dir = \"/kaggle/working/\"\nos.makedirs(base_dir, exist_ok=True)\n\noutput_dir = os.path.join(base_dir, \"outputs\")\nplots_dir = os.path.join(output_dir, \"plots\")\ncheckpoint_dir = os.path.join(output_dir, \"checkpoints\")\nbase_model_dir = os.path.join(output_dir, \"gpt2_xl_sft_rlhf_base\")\nreward_base_model_dir = os.path.join(output_dir, \"qwen_reward_base\")\nreward_trained_model_dir = os.path.join(output_dir, \"qwen_reward_trained\")\nsft_model_save_path = os.path.join(output_dir, \"sft_model\")\nppo_model_save_path = os.path.join(output_dir, \"ppo_model\")\n\nfor dir_path in [output_dir, plots_dir, checkpoint_dir, base_model_dir, reward_base_model_dir, reward_trained_model_dir, sft_model_save_path, ppo_model_save_path]:\n    os.makedirs(dir_path, exist_ok=True)\n\n# # %% Cleanup checkpoints directory\n# if os.path.exists(checkpoint_dir):\n#     shutil.rmtree(checkpoint_dir)\n#     print(f\"Deleted the directory: {checkpoint_dir}\")\n# else:\n#     print(f\"The directory {checkpoint_dir} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:29:23.632624Z","iopub.execute_input":"2025-05-18T17:29:23.633572Z","iopub.status.idle":"2025-05-18T17:29:23.643395Z","shell.execute_reply.started":"2025-05-18T17:29:23.633544Z","shell.execute_reply":"2025-05-18T17:29:23.642719Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# # %% Cleanup checkpoints directory\n# if os.path.exists(checkpoint_dir):\n#     shutil.rmtree(checkpoint_dir)\n#     print(f\"Deleted the directory: {checkpoint_dir}\")\n# else:\n#     print(f\"The directory {checkpoint_dir} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:53:25.857937Z","iopub.execute_input":"2025-05-18T16:53:25.858348Z","iopub.status.idle":"2025-05-18T16:53:25.902417Z","shell.execute_reply.started":"2025-05-18T16:53:25.858322Z","shell.execute_reply":"2025-05-18T16:53:25.901580Z"}},"outputs":[{"name":"stdout","text":"Deleted the directory: /kaggle/working/outputs/checkpoints\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# import os\n\n# # Define the base directory and output directory\n# base_dir = \"/kaggle/working/\"\n# output_dir = os.path.join(base_dir, \"outputs\")\n# checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n\n# # Create the output directory if it doesn't exist\n# os.makedirs(output_dir, exist_ok=True)\n\n# # Check if the checkpoint directory exists\n# if not os.path.exists(checkpoint_dir):\n#     os.makedirs(checkpoint_dir)\n#     print(f\"Checkpoint directory created at: {checkpoint_dir}\")\n# else:\n#     print(f\"Checkpoint directory already exists at: {checkpoint_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T16:57:27.554777Z","iopub.execute_input":"2025-05-18T16:57:27.555634Z","iopub.status.idle":"2025-05-18T16:57:27.561772Z","shell.execute_reply.started":"2025-05-18T16:57:27.555599Z","shell.execute_reply":"2025-05-18T16:57:27.560933Z"}},"outputs":[{"name":"stdout","text":"Checkpoint directory created at: /kaggle/working/outputs/checkpoints\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# %% Load and Prepare Datasets\ndef load_jsonl(file_path):\n    data = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:29:49.305575Z","iopub.execute_input":"2025-05-18T17:29:49.306149Z","iopub.status.idle":"2025-05-18T17:29:49.309715Z","shell.execute_reply.started":"2025-05-18T17:29:49.306126Z","shell.execute_reply":"2025-05-18T17:29:49.309150Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"sft_raw_data = load_jsonl(\"/kaggle/input/mini-pipeline-datasets/sft_raw_data.jsonl\")\nrlhf_raw_data = load_jsonl(\"/kaggle/input/mini-pipeline-datasets/reformatted_rlhf_data.jsonl\")\nreward_raw_data = load_jsonl(\"/kaggle/input/mini-pipeline-datasets/reformatted_reward_data.jsonl\")\n\ntrain_sft_data, val_sft_data = train_test_split(sft_raw_data, test_size=0.1, random_state=42)\nsft_dataset = Dataset.from_list(train_sft_data)\nval_dataset = Dataset.from_list(val_sft_data)\n\nrlhf_dataset = Dataset.from_list(rlhf_raw_data)\ntrain_rm_data, val_rm_data = train_test_split(reward_raw_data, test_size=0.1, random_state=42)\ntrain_rm_dataset = Dataset.from_list(train_rm_data)\nval_rm_dataset = Dataset.from_list(val_rm_data)\nreward_val = Dataset.from_list(reward_raw_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:29:51.208846Z","iopub.execute_input":"2025-05-18T17:29:51.209104Z","iopub.status.idle":"2025-05-18T17:29:51.322941Z","shell.execute_reply.started":"2025-05-18T17:29:51.209086Z","shell.execute_reply":"2025-05-18T17:29:51.322211Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# %% Helper function to extract boxed answer\ndef extract_boxed_answer(text):\n    matches = re.findall(r\"\\\\boxed{([^}]*)}\", text)\n    if matches:\n        return matches[-1].strip()\n    fallback = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", text)\n    return fallback[-1].strip() if fallback else text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:29:53.500440Z","iopub.execute_input":"2025-05-18T17:29:53.501134Z","iopub.status.idle":"2025-05-18T17:29:53.504989Z","shell.execute_reply.started":"2025-05-18T17:29:53.501107Z","shell.execute_reply":"2025-05-18T17:29:53.504257Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Remove Unsloth Cache\nunsloth_cache = os.path.expanduser(\"~/.cache/huggingface/\")\ntry:\n    for f in os.listdir(unsloth_cache):\n        if \"unsloth\" in f:\n            path = os.path.join(unsloth_cache, f)\n            if os.path.isfile(path):\n                os.remove(path)\n            else:\n                shutil.rmtree(path)\nexcept Exception as e:\n    print(f\"Cache cleanup warning: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:30:30.416494Z","iopub.execute_input":"2025-05-18T17:30:30.417015Z","iopub.status.idle":"2025-05-18T17:30:30.421340Z","shell.execute_reply.started":"2025-05-18T17:30:30.416990Z","shell.execute_reply":"2025-05-18T17:30:30.420819Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nimport torch\nfrom unsloth import FastLanguageModel\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom transformers import GenerationConfig, AutoConfig\n\n# Define base_model_dir\nbase_model_dir = \"/kaggle/working/outputs/gpt2_xl_sft_rlhf_base\"  # Adjust as needed\n\nmax_seq_length = 256  # Reduced for memory\ndtype = torch.float16  # Use float16 for compatibility\nload_in_4bit = False  # Explicitly disable 4-bit quantization\n\nsft_rlhf_model_name = \"gpt2-xl\"\n\nprint(f\"Loading SFT/RLHF Model: {sft_rlhf_model_name}\")\n\ntry:\n    if os.path.exists(base_model_dir) and os.path.exists(os.path.join(base_model_dir, \"config.json\")):\n        print(f\"Loading GPT-2 XL from {base_model_dir}\")\n        # Load configuration to verify\n        model_config = AutoConfig.from_pretrained(base_model_dir)\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            base_model_dir,\n            max_seq_length=max_seq_length,\n            dtype=dtype,\n            load_in_4bit=load_in_4bit\n        )\n    else:\n        print(f\"Downloading GPT-2 XL from Hugging Face and saving to {base_model_dir}\")\n        # Create directory if it doesn't exist\n        os.makedirs(base_model_dir, exist_ok=True)\n        model_config = AutoConfig.from_pretrained(sft_rlhf_model_name)\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            sft_rlhf_model_name,\n            max_seq_length=max_seq_length,\n            dtype=dtype,\n            load_in_4bit=load_in_4bit\n        )\n        model.save_pretrained(base_model_dir)\n        tokenizer.save_pretrained(base_model_dir)\n        print(f\"✅ Successfully downloaded and saved GPT-2 XL to {base_model_dir}\")\n        print(f\"Contents of {base_model_dir}: {os.listdir(base_model_dir)}\")\n\nexcept TypeError as e:\n    print(f\"\\n❌ Error during model loading: {e}\")\n    print(\"This often indicates a corrupted or incomplete model configuration.\")\n    print(f\"Please try deleting the local model directory '{base_model_dir}' and run the script again to force a fresh download.\")\n    print(\"If the issue persists, consider using transformers.AutoModelForCausalLM directly as a fallback.\")\n    exit(1)  # Exit the script if model loading fails\n\nexcept Exception as e:\n    print(f\"\\n❌ Unexpected error during model loading: {e}\")\n    print(f\"Please check network connectivity, disk space, or compatibility with {sft_rlhf_model_name}.\")\n    print(f\"Try deleting '{base_model_dir}' and rerun.\")\n    exit(1)\n\n# Move model to GPU\nif torch.cuda.is_available():\n    model = model.to(\"cuda:0\")\n    print(f\"Model moved to GPU: {next(model.parameters()).device}\")\n\n# Set pad token for tokenizer\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.pad_token_id\n    print(f\"Tokenizer pad token set to: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n\n# Apply LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"c_attn\", \"c_proj\"],  # Suitable for GPT-2\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nFastLanguageModel.for_training(model)\n\n# Set generation config with specific parameters\nmodel.generation_config = GenerationConfig(\n    max_length=max_seq_length,\n    pad_token_id=model.config.pad_token_id,\n    eos_token_id=model.config.eos_token_id\n)\n\nprint(\"✅ SFT/RLHF Model is ready for fine-tuning with LoRA applied (no quantization).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:30:32.870835Z","iopub.execute_input":"2025-05-18T17:30:32.871099Z","iopub.status.idle":"2025-05-18T17:31:16.061329Z","shell.execute_reply.started":"2025-05-18T17:30:32.871080Z","shell.execute_reply":"2025-05-18T17:31:16.060451Z"}},"outputs":[{"name":"stdout","text":"Loading SFT/RLHF Model: gpt2-xl\nDownloading GPT-2 XL from Hugging Face and saving to /kaggle/working/outputs/gpt2_xl_sft_rlhf_base\n==((====))==  Unsloth 2025.5.6: Fast Gpt2 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4995a6afa8f64e3382fb9ae35143c842"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f77de04bde2471ca9e824c5b93a3a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9afe907408384398aed7423065fff1d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865c63371ff14845be5baba60e1d2cf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b86757cee7ab4283b1e3e5f4030640f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0350c4084e4a497097247eb8296e88c8"}},"metadata":{}},{"name":"stdout","text":"gpt2-xl does not have a padding token! Will use pad_token = <|endoftext|>.\n✅ Successfully downloaded and saved GPT-2 XL to /kaggle/working/outputs/gpt2_xl_sft_rlhf_base\nContents of /kaggle/working/outputs/gpt2_xl_sft_rlhf_base: ['tokenizer_config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'vocab.json', 'config.json', 'special_tokens_map.json', 'tokenizer.json']\nModel moved to GPU: cuda:0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ SFT/RLHF Model is ready for fine-tuning with LoRA applied (no quantization).\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# %% Step 3: Supervised Fine-Tuning (SFT)\ndef tokenize_function_sft(examples):\n    inputs = tokenizer(\n        [f\"Problem: {p}\\nSolution: {s}\" for p, s in zip(examples[\"problem\"], examples[\"solution\"])],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_seq_length,\n        return_tensors=\"pt\"\n    )\n    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n    return inputs\n\nsft_dataset = sft_dataset.map(tokenize_function_sft, batched=True)\nval_dataset = val_dataset.map(tokenize_function_sft, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:31:34.563821Z","iopub.execute_input":"2025-05-18T17:31:34.564580Z","iopub.status.idle":"2025-05-18T17:31:34.868954Z","shell.execute_reply.started":"2025-05-18T17:31:34.564555Z","shell.execute_reply":"2025-05-18T17:31:34.868210Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/360 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6837b085111e4a69993f787ea4aef40c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9b9de1773142269d3060215a82444e"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from transformers import Trainer\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, processing_class=None, **kwargs):\n        # Handle deprecated 'tokenizer' argument for compatibility\n        if \"tokenizer\" in kwargs:\n            processing_class = kwargs.pop(\"tokenizer\")\n        super().__init__(*args, processing_class=processing_class, **kwargs)\n        self.train_losses = []\n        self.val_accuracies = []\n        self.val_reasonings = []\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        # Ignore num_items_in_batch since we don't need it for loss computation\n        outputs = model(**inputs)\n        loss = outputs.loss\n        self.train_losses.append(loss.item())\n        return (loss, outputs) if return_outputs else loss\n\n    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n        accuracy, reasoning = evaluate_model(reward_val, self.model, self.tokenizer)\n        self.val_accuracies.append(accuracy)\n        self.val_reasonings.append(reasoning)\n        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n        metrics[f\"{metric_key_prefix}_accuracy\"] = accuracy\n        metrics[f\"{metric_key_prefix}_reasoning\"] = reasoning\n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:31:37.633779Z","iopub.execute_input":"2025-05-18T17:31:37.634365Z","iopub.status.idle":"2025-05-18T17:31:37.640602Z","shell.execute_reply.started":"2025-05-18T17:31:37.634343Z","shell.execute_reply":"2025-05-18T17:31:37.639906Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# %% Find latest checkpoint\ndef find_latest_checkpoint(checkpoint_dir):\n    checkpoint_paths = glob.glob(os.path.join(checkpoint_dir, \"checkpoint-*\"))\n    return checkpoint_paths[-1] if checkpoint_paths else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:31:40.108575Z","iopub.execute_input":"2025-05-18T17:31:40.108825Z","iopub.status.idle":"2025-05-18T17:31:40.112642Z","shell.execute_reply.started":"2025-05-18T17:31:40.108808Z","shell.execute_reply":"2025-05-18T17:31:40.112057Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# %% SFT Training\nfrom transformers import TrainingArguments\nfrom peft import PeftModel\n\nsft_args = TrainingArguments(\n    output_dir=checkpoint_dir,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    warmup_steps=10,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",\n    save_steps=50,\n    load_best_model_at_end=True,\n    fp16=True,\n    weight_decay=0.01,\n    report_to=\"none\",\n    save_total_limit=3,\n    gradient_checkpointing=False\n)\n\ntrainer = CustomTrainer(\n    model=model,\n    args=sft_args,\n    train_dataset=sft_dataset,\n    eval_dataset=val_dataset,\n    processing_class=tokenizer  # Use processing_class as fixed earlier\n)\n\nlatest_checkpoint = find_latest_checkpoint(checkpoint_dir)\nprint(\"Starting SFT...\")\ntrainer.train(resume_from_checkpoint=latest_checkpoint)\n\n# Merge LoRA adapters and save the full model\nprint(f\"Merging LoRA adapters and saving full SFT model to {sft_model_save_path}\")\nif isinstance(model, PeftModel):\n    # Get the base model and merge the adapters\n    model = model.merge_and_unload()  # Merges LoRA weights into the base model\nelse:\n    print(\"Model is not a PeftModel; saving as-is.\")\n\n# Save the full model (now includes base weights and config)\nmodel.save_pretrained(sft_model_save_path, torch_dtype=torch.float16)\ntokenizer.save_pretrained(sft_model_save_path)\nprint(\"SFT model saved. Contents:\", os.listdir(sft_model_save_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:31:42.095544Z","iopub.execute_input":"2025-05-18T17:31:42.095801Z","iopub.status.idle":"2025-05-18T17:33:48.338697Z","shell.execute_reply.started":"2025-05-18T17:31:42.095785Z","shell.execute_reply":"2025-05-18T17:33:48.337749Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 360 | Num Epochs = 1 | Total steps = 22\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 13,516,800/1,571,128,000 (0.86% trained)\n","output_type":"stream"},{"name":"stdout","text":"Starting SFT...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [22/22 01:34, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Merging LoRA adapters and saving full SFT model to /kaggle/working/outputs/sft_model\nSFT model saved. Contents: ['model.safetensors.index.json', 'tokenizer_config.json', 'generation_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'vocab.json', 'config.json', 'special_tokens_map.json', 'tokenizer.json']\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# %% Plot SFT Metrics\nplt.figure(figsize=(10, 6))\nplt.plot([i * sft_args.logging_steps for i in range(len(trainer.train_losses))], trainer.train_losses, label=\"Training Loss\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"SFT Training Loss\")\nplt.legend()\nplt.savefig(os.path.join(plots_dir, \"sft_training_loss.png\"))\nplt.close()\n\nplt.figure(figsize=(10, 6))\nplt.plot([i * sft_args.eval_steps for i in range(len(trainer.val_accuracies))], trainer.val_accuracies, marker='o', label=\"Validation Accuracy\")\nplt.plot([i * sft_args.eval_steps for i in range(len(trainer.val_reasonings))], trainer.val_reasonings, marker='x', label=\"Validation Reasoning Quality\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Score\")\nplt.title(\"SFT Validation Metrics\")\nplt.legend()\nplt.savefig(os.path.join(plots_dir, \"sft_validation_metrics.png\"))\nplt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:34:52.527662Z","iopub.execute_input":"2025-05-18T17:34:52.528465Z","iopub.status.idle":"2025-05-18T17:34:52.944661Z","shell.execute_reply.started":"2025-05-18T17:34:52.528433Z","shell.execute_reply":"2025-05-18T17:34:52.943761Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enable for debugging\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:05:37.668344Z","iopub.execute_input":"2025-05-18T17:05:37.669092Z","iopub.status.idle":"2025-05-18T17:05:37.710396Z","shell.execute_reply.started":"2025-05-18T17:05:37.669067Z","shell.execute_reply":"2025-05-18T17:05:37.709497Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# %% Step 4: Reward Model Training (with Disk Cleanup)\nimport os\nimport torch\nimport gc\nimport shutil\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\nfrom peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n\n# Set PyTorch memory configuration to reduce fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Check number of available GPUs\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of available GPUs: {num_gpus}\")\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# Check and display initial disk space\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Initial disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# Aggressive memory and disk cleanup\nprint(\"Performing aggressive cleanup before loading reward model...\")\n# Delete all known references to previous objects\nglobal_vars_to_delete = [\n    'model', 'trainer', 'sft_dataset', 'val_dataset', 'sft_args',\n    'tokenized_train_rm_dataset', 'tokenized_val_rm_dataset'\n]\nfor var in global_vars_to_delete:\n    if var in globals():\n        print(f\"Deleting {var}...\")\n        del globals()[var]\n\n# Delete GPT-2 XL base model if present (adjust path if different)\ngpt2_base_dir = os.path.join(\"/kaggle/working/outputs\", \"gpt2-xl-base\")  # Adjust path if needed\nif os.path.exists(gpt2_base_dir):\n    shutil.rmtree(gpt2_base_dir)\n    print(f\"Deleted GPT-2 XL base model from {gpt2_base_dir}\")\n\n# Delete old checkpoints to free space\ncheckpoint_dir = \"/kaggle/working/outputs/checkpoints\"\nif os.path.exists(checkpoint_dir):\n    checkpoints = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n    if len(checkpoints) > 1:\n        for chk in checkpoints[:-1]:  # Keep the latest checkpoint\n            shutil.rmtree(os.path.join(checkpoint_dir, chk))\n            print(f\"Deleted checkpoint: {chk}\")\n\n# GPU memory cleanup\ntorch.cuda.reset_peak_memory_stats()\nfor _ in range(3):\n    torch.cuda.empty_cache()\n    gc.collect()\n    torch.cuda.synchronize()\n\n# Check memory and disk usage after cleanup\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated after cleanup: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved after cleanup: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage after cleanup - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\nif any(torch.cuda.memory_allocated(i) / 1024**2 > 5000 for i in range(num_gpus)):\n    print(\"⚠️ Warning: GPU memory usage is still high after cleanup.\")\n    print(\"Consider restarting the kernel and running only the reward model training step.\")\n\nprint(f\"\\n⚡️ Starting Reward Model Training (Regression)\\n\")\n\nreward_model_name_hub = \"Qwen/Qwen2.5-1.5B-Instruct\"\nmax_seq_length = 64\n\nconfig_path = os.path.join(reward_base_model_dir, \"config.json\")\nloaded_from_local = False\n\nif os.path.exists(config_path):\n    try:\n        print(f\"✅ Attempting to load Base Reward Model from {reward_base_model_dir}\")\n        reward_model = AutoModelForSequenceClassification.from_pretrained(\n            reward_base_model_dir,\n            num_labels=1,\n            load_in_4bit=False,\n            trust_remote_code=True,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n        reward_tokenizer = AutoTokenizer.from_pretrained(reward_base_model_dir, trust_remote_code=True)\n        loaded_from_local = True\n        print(\"✅ Successfully loaded Base Reward Model from local path.\")\n        print(f\"Device map: {reward_model.hf_device_map if hasattr(reward_model, 'hf_device_map') else 'Not available'}\")\n    except Exception as e:\n        print(f\"❌ Failed to load Base Reward Model from {reward_base_model_dir}: {e}\")\n\nif not loaded_from_local:\n    print(f\"⏳ Base Reward Model not found locally or corrupted. Downloading from {reward_model_name_hub}\")\n    try:\n        reward_model = AutoModelForSequenceClassification.from_pretrained(\n            reward_model_name_hub,\n            num_labels=1,\n            load_in_4bit=False,\n            trust_remote_code=True,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n        reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name_hub, trust_remote_code=True)\n        print(f\"💾 Saving downloaded Base Reward Model to {reward_base_model_dir}\")\n        reward_model.save_pretrained(reward_base_model_dir)\n        reward_tokenizer.save_pretrained(reward_base_model_dir)\n        print(\"✅ Successfully downloaded and saved Base Reward Model.\")\n        print(f\"Device map: {reward_model.hf_device_map if hasattr(reward_model, 'hf_device_map') else 'Not available'}\")\n    except Exception as e:\n        print(f\"Fatal Error: Could not download or load Base Reward Model: {e}\")\n        raise e\n\nreward_model = prepare_model_for_kbit_training(reward_model, use_gradient_checkpointing=True)\n\nreward_lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_CLS\n)\n\nreward_model = get_peft_model(reward_model, reward_lora_config)\n\nif reward_tokenizer.pad_token is None:\n    reward_tokenizer.pad_token = reward_tokenizer.eos_token if reward_tokenizer.eos_token else \"[PAD]\"\n    print(f\"Reward Tokenizer pad token set to: {reward_tokenizer.pad_token}\")\n\npad_token_id = reward_tokenizer.convert_tokens_to_ids(reward_tokenizer.pad_token)\nif reward_model.config.pad_token_id is None or reward_model.config.pad_token_id == -1:\n    reward_model.config.pad_token_id = pad_token_id\n    print(f\"Reward Model config pad_token_id set to: {reward_model.config.pad_token_id}\")\n\ndef preprocess_reward_dataset_for_rm_regression(examples, tokenizer):\n    texts = [f\"Problem: {p}\\nSolution: {s}\" for p, s in zip(examples[\"problem\"], examples[\"solution\"])]\n    labels = [float(l) for l in [item for sublist in examples[\"simulated_rewards\"] for item in sublist]]\n    tokenized_inputs = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_seq_length)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_train_rm_dataset = train_rm_dataset.map(\n    lambda x: preprocess_reward_dataset_for_rm_regression(x, reward_tokenizer),\n    batched=True,\n    remove_columns=[col for col in train_rm_dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']]\n)\ntokenized_val_rm_dataset = val_rm_dataset.map(\n    lambda x: preprocess_reward_dataset_for_rm_regression(x, reward_tokenizer),\n    batched=True,\n    remove_columns=[col for col in val_rm_dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']]\n)\n\n# Memory and disk check after loading model\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated after loading reward model: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage before training - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\nreward_args = TrainingArguments(\n    output_dir=checkpoint_dir,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",\n    save_steps=50,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-5,\n    weight_decay=0.001,\n    report_to=\"none\",\n    save_total_limit=1,  # Limit to 1 checkpoint to save disk space\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True\n)\n\nreward_trainer = Trainer(\n    model=reward_model,\n    tokenizer=reward_tokenizer,\n    args=reward_args,\n    train_dataset=tokenized_train_rm_dataset,\n    eval_dataset=tokenized_val_rm_dataset,\n    data_collator=DataCollatorWithPadding(reward_tokenizer, padding=\"max_length\", max_length=max_seq_length)\n)\n\nprint(\"Starting Reward Model Training...\")\nreward_trainer.train()\nprint(f\"✅ Reward Model Training Completed and Saved to {reward_trained_model_dir}\")\n\n# Check disk space before saving the final model\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage before final save - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# Merge LoRA adapters and save the full model\nprint(f\"Merging LoRA adapters and saving full reward model to {reward_trained_model_dir}\")\nif isinstance(reward_model, PeftModel):\n    reward_model = reward_model.merge_and_unload()\nelse:\n    print(\"Reward model is not a PeftModel; saving as-is.\")\nreward_model.save_pretrained(reward_trained_model_dir, torch_dtype=torch.float16)\nreward_tokenizer.save_pretrained(reward_trained_model_dir)\nprint(\"Reward model saved. Contents:\", os.listdir(reward_trained_model_dir))\n\n# Clear GPU memory after training\nprint(\"Clearing GPU memory after reward model training...\")\ntorch.cuda.empty_cache()\ndel reward_model\ndel reward_trainer\ndel tokenized_train_rm_dataset\ndel tokenized_val_rm_dataset\ngc.collect()\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T17:35:04.418643Z","iopub.execute_input":"2025-05-18T17:35:04.418939Z","iopub.status.idle":"2025-05-18T17:37:07.260501Z","shell.execute_reply.started":"2025-05-18T17:35:04.418919Z","shell.execute_reply":"2025-05-18T17:37:07.259794Z"}},"outputs":[{"name":"stdout","text":"Number of available GPUs: 2\nGPU 0 memory allocated: 6228.20 MB\nGPU 0 memory reserved: 7298.00 MB\nGPU 1 memory allocated: 2.00 MB\nGPU 1 memory reserved: 20.00 MB\nInitial disk usage - Total: 19.52 GB, Used: 8.87 GB, Free: 10.63 GB\nPerforming aggressive cleanup before loading reward model...\nDeleting model...\nDeleting trainer...\nDeleting sft_dataset...\nDeleting val_dataset...\nDeleting sft_args...\nGPU 0 memory allocated after cleanup: 19.38 MB\nGPU 0 memory reserved after cleanup: 2998.00 MB\nGPU 1 memory allocated after cleanup: 2.00 MB\nGPU 1 memory reserved after cleanup: 20.00 MB\nDisk usage after cleanup - Total: 19.52 GB, Used: 8.87 GB, Free: 10.63 GB\n\n⚡️ Starting Reward Model Training (Regression)\n\n⏳ Base Reward Model not found locally or corrupted. Downloading from Qwen/Qwen2.5-1.5B-Instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125f7a49f5c34a2e9326f290f2c2973a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004ed16e57a54588830e0987386aabc8"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nSome weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1e870906974912a8101f029f85b38b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdbc992801274a7da6a8ccb8dd54818c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"428ad08aca5f4154816ec08ed4a43151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b112c7eeca98478fa04bdfe2ef70a35c"}},"metadata":{}},{"name":"stdout","text":"💾 Saving downloaded Base Reward Model to /kaggle/working/outputs/qwen_reward_base\n✅ Successfully downloaded and saved Base Reward Model.\nDevice map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'score': 1}\nReward Model config pad_token_id set to: 151643\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/191 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23a81ab84f447ceaed5860ef9804444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329be48bd9a1453bba653ad5e83acfc9"}},"metadata":{}},{"name":"stdout","text":"GPU 0 memory allocated after loading reward model: 2902.34 MB\nGPU 0 memory reserved: 3136.00 MB\nGPU 1 memory allocated after loading reward model: 3079.62 MB\nGPU 1 memory reserved: 3420.00 MB\nDisk usage before training - Total: 19.52 GB, Used: 11.76 GB, Free: 7.74 GB\nStarting Reward Model Training...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/1341984294.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  reward_trainer = Trainer(\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n   \\\\   /|    Num examples = 191 | Num Epochs = 1 | Total steps = 191\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n \"-____-\"     Trainable parameters = 18,466,304/1,562,182,144 (1.18% trained)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [191/191 01:05, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>4.488800</td>\n      <td>4.251545</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>8.300800</td>\n      <td>3.955332</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.717100</td>\n      <td>3.818267</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"✅ Reward Model Training Completed and Saved to /kaggle/working/outputs/qwen_reward_trained\nDisk usage before final save - Total: 19.52 GB, Used: 11.83 GB, Free: 7.68 GB\nMerging LoRA adapters and saving full reward model to /kaggle/working/outputs/qwen_reward_trained\nReward model saved. Contents: ['model.safetensors.index.json', 'tokenizer_config.json', 'merges.txt', 'model-00001-of-00002.safetensors', 'model-00002-of-00002.safetensors', 'vocab.json', 'config.json', 'special_tokens_map.json', 'tokenizer.json', 'added_tokens.json']\nClearing GPU memory after reward model training...\nGPU 0 memory allocated: 19.38 MB\nGPU 0 memory reserved: 3208.00 MB\nGPU 1 memory allocated: 18.38 MB\nGPU 1 memory reserved: 3556.00 MB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport torch\nimport gc\nimport shutil\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    AutoModelForSequenceClassification\n)\nfrom peft import PeftModel\nfrom trl import PPOTrainer, PPOConfig\nfrom datasets import load_dataset\n\n# Set PyTorch memory configuration to reduce fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Check number of available GPUs\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of available GPUs: {num_gpus}\")\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# Check and display initial disk space\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Initial disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# Aggressive memory and disk cleanup\nprint(\"Performing aggressive cleanup before starting RLHF...\")\n# Delete all known references to previous objects\nglobal_vars_to_delete = [\n    'model', 'trainer', 'sft_dataset', 'val_dataset', 'sft_args',\n    'tokenized_train_rm_dataset', 'tokenized_val_rm_dataset',\n    'reward_model', 'reward_trainer'\n]\nfor var in global_vars_to_delete:\n    if var in globals():\n        print(f\"Deleting {var}...\")\n        del globals()[var]\n\n# Delete the base Reward Model to free space\nreward_base_model_dir = \"/kaggle/working/outputs/qwen_reward_base\"\nif os.path.exists(reward_base_model_dir):\n    shutil.rmtree(reward_base_model_dir)\n    print(f\"Deleted base Reward Model from {reward_base_model_dir}\")\n\n# Delete old checkpoints to free space\ncheckpoint_dir = \"/kaggle/working/outputs/checkpoints\"\nif os.path.exists(checkpoint_dir):\n    checkpoints = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n    if len(checkpoints) > 1:\n        for chk in checkpoints[:-1]:  # Keep the latest checkpoint\n            shutil.rmtree(os.path.join(checkpoint_dir, chk))\n            print(f\"Deleted checkpoint: {chk}\")\n\n# GPU memory cleanup\ntorch.cuda.reset_peak_memory_stats()\nfor _ in range(3):\n    torch.cuda.empty_cache()\n    gc.collect()\n    torch.cuda.synchronize()\n\n# Check memory and disk usage after cleanup\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated after cleanup: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved after cleanup: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage after cleanup - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\nprint(f\"\\n⚡️ Starting Reinforcement Learning Fine-Tuning (RLHF)\\n\")\n\n# Determine the device to load models on\n# By explicitly setting a single device, we avoid potential cross-GPU tensor issues.\n# Ensure the chosen GPU has enough VRAM for the entire model (approx. 6GB in float16 for your setup).\ntarget_device = \"cuda:0\" # You can change this to \"cuda:1\" if GPU 1 is preferred or has more space\n\n# Load SFT-trained model\nsft_model_save_path = \"/kaggle/working/outputs/sft_model\"\nprint(f\"Loading SFT-trained model in float16 from {sft_model_save_path}\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    sft_model_save_path,\n    torch_dtype=torch.float16,\n    device_map=target_device,  # FIX: Explicitly place on one GPU\n    attn_implementation=\"eager\"\n)\nif os.path.exists(os.path.join(sft_model_save_path, \"adapter_model.safetensors\")):\n    # If adapter exists, ensure it's also loaded onto the same device\n    model = PeftModel.from_pretrained(model, sft_model_save_path, torch_dtype=torch.float16, device_map=target_device) # FIX: Explicitly place on one GPU\nfor name, param in model.named_parameters():\n    if \"lora\" in name.lower():\n        param.data = param.data.to(torch.float16)\n        print(f\"Cast {name} to float16\")\ntokenizer = AutoTokenizer.from_pretrained(sft_model_save_path)\ntokenizer.padding_side = 'left'\nif tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\nmodel.config.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\nprint(f\"Model config - pad_token_id: {model.config.pad_token_id}, eos_token_id: {model.config.eos_token_id}\")\n\n# Load the trained Reward Model\nreward_trained_model_dir = \"/kaggle/working/outputs/qwen_reward_trained\"\nprint(f\"Loading trained Reward Model from {reward_trained_model_dir}\")\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n    reward_trained_model_dir,\n    num_labels=1,\n    torch_dtype=torch.float16,\n    device_map=target_device,  # FIX: Explicitly place on one GPU\n    trust_remote_code=True\n)\nreward_tokenizer = AutoTokenizer.from_pretrained(reward_trained_model_dir, trust_remote_code=True)\n\n# Memory and disk check after loading models\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated after loading models: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage after loading models - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# Mock train_rm_dataset if it's not defined\n# This part is crucial for the script to be runnable standalone for testing purposes.\nif 'train_rm_dataset' not in globals():\n    print(\"train_rm_dataset not found, creating a mock dataset for demonstration.\")\n    from datasets import Dataset\n    # Create a dummy dataset with the expected 'problem' column\n    dummy_data = {\n        \"problem\": [\n            \"What is 2+2?\",\n            \"Solve for x: x - 5 = 10\",\n            \"What is the capital of France?\",\n            \"Calculate the area of a rectangle with length 5 and width 3.\"\n        ]\n    }\n    train_rm_dataset = Dataset.from_dict(dummy_data)\n    print(f\"Mock train_rm_dataset created with {len(train_rm_dataset)} examples.\")\n\n# Prepare dataset for RLHF\nmax_seq_length = 64\ndef preprocess_rlhf_dataset(examples, tokenizer):\n    prompts = [f\"Problem: {p}\\nSolution: \" for p in examples[\"problem\"]]\n    tokenized_inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\")\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"]}\n\nrlhf_dataset = train_rm_dataset.map(\n    lambda x: preprocess_rlhf_dataset(x, tokenizer),\n    batched=True,\n    remove_columns=[col for col in train_rm_dataset.column_names if col not in ['input_ids', 'attention_mask']]\n)\n\n# Configure PPO for RLHF\nppo_config = PPOConfig(\n    batch_size=1,\n    learning_rate=1e-5,\n    num_ppo_epochs=1,\n    mini_batch_size=1,\n    gradient_accumulation_steps=1,\n    #log_with=None\n    ref_model_backend=\"cpu\", # FIX: Offload the reference model to CPU\n)\n\n# Initialize PPO Trainer\nppo_trainer = PPOTrainer(\n    args=ppo_config,\n    model=model,\n    ref_model=None,         # trl will manage the reference model based on ppo_config.ref_model_backend\n    tokenizer=tokenizer,\n    train_dataset=rlhf_dataset,\n    reward_model=reward_model,\n    value_model=model,      # Pass the 'model' itself as the value model\n    data_collator=DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_seq_length)\n)\nprint(\"PPOTrainer initialized.\")\n\n# RLHF Training Loop\nprint(\"Starting RLHF training with PPO...\")\nfor step, batch in enumerate(ppo_trainer.dataloader):\n    # Ensure input tensors are on the same device as the model\n    query_tensors = batch[\"input_ids\"].to(target_device)\n    attention_mask = batch[\"attention_mask\"].to(target_device)\n\n    # Generate responses\n    response_tensors = ppo_trainer.model.policy.generate(\n        query_tensors,\n        attention_mask=attention_mask,\n        max_new_tokens=64, # Use max_new_tokens to specify how many NEW tokens to generate.\n                           # Be aware: The total generated sequence length (input_length + max_new_tokens)\n                           # might exceed the max_seq_length (64) used by reward_tokenizer,\n                           # leading to truncation for reward calculation.\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        num_return_sequences=1\n    )\n\n    # Prepare inputs for the Reward Model\n    queries = [tokenizer.decode(q, skip_special_tokens=True) for q in query_tensors]\n    responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n    \n    # Extract the problem part from each query\n    reward_inputs = []\n    for q, r in zip(queries, responses):\n        # Split the query to isolate the problem part\n        problem_part = q.split('Problem: ')[1].split('\\nSolution: ')[0] if 'Problem: ' in q and '\\nSolution: ' in q else q\n        \n        # Construct the reward input in the required format\n        reward_input = f\"Problem: {problem_part}\\nSolution: {r}\"\n        reward_inputs.append(reward_input)\n\n    # Tokenize for reward model. Ensure inputs are on the same device as the reward model.\n    reward_tokenized = reward_tokenizer(reward_inputs, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\").to(target_device)\n\n    # Get rewards\n    with torch.no_grad():\n        reward_outputs = reward_model(**reward_tokenized)\n        rewards = reward_outputs.logits.squeeze()\n        if rewards.ndim == 0: # Ensure it's a 1-dim tensor if batch_size=1\n            rewards = rewards.unsqueeze(0)\n\n    # PPO step\n    stats = ppo_trainer.step(\n        query_tensors,\n        response_tensors,\n        rewards\n    )\n\n    if step % 10 == 0:\n        print(f\"Step {step} - PPO stats: {stats}\")\n\n    if step >= 50:  # Limit for demonstration; adjust as needed\n        print(f\"Reached step {step}, stopping training.\")\n        break\n\n# Save the RLHF-trained model\nrlhf_model_save_path = \"/kaggle/working/outputs/rlhf_model\"\nprint(f\"Saving RLHF-trained model to {rlhf_model_save_path}\")\nmodel.save_pretrained(rlhf_model_save_path, torch_dtype=torch.float16)\ntokenizer.save_pretrained(rlhf_model_save_path)\nprint(\"RLHF model saved. Contents:\", os.listdir(rlhf_model_save_path))\n\n# Clear GPU memory after training\nprint(\"Clearing GPU memory after RLHF training...\")\ntorch.cuda.empty_cache()\ndel model\ndel reward_model\ndel ppo_trainer\ndel rlhf_dataset\ngc.collect()\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# Final disk usage check\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Final disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:05:13.573219Z","iopub.execute_input":"2025-05-18T18:05:13.573899Z","iopub.status.idle":"2025-05-18T18:05:15.605330Z","shell.execute_reply.started":"2025-05-18T18:05:13.573873Z","shell.execute_reply":"2025-05-18T18:05:15.604248Z"}},"outputs":[{"name":"stdout","text":"Number of available GPUs: 2\nGPU 0 memory allocated: 14675.27 MB\nGPU 0 memory reserved: 14892.00 MB\nGPU 1 memory allocated: 19.16 MB\nGPU 1 memory reserved: 1562.00 MB\nInitial disk usage - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\nPerforming aggressive cleanup before starting RLHF...\nDeleting model...\nDeleting reward_model...\nGPU 0 memory allocated after cleanup: 14620.95 MB\nGPU 0 memory reserved after cleanup: 14852.00 MB\nGPU 1 memory allocated after cleanup: 18.38 MB\nGPU 1 memory reserved after cleanup: 1558.00 MB\nDisk usage after cleanup - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\n\n⚡️ Starting Reinforcement Learning Fine-Tuning (RLHF)\n\nLoading SFT-trained model in float16 from /kaggle/working/outputs/sft_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96696849bf124637b661544d65d57b4b"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2578591841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0msft_model_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/outputs/sft_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading SFT-trained model in float16 from {sft_model_save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0msft_model_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4397\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4398\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4399\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4400\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4401\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4831\u001b[0m             \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4833\u001b[0;31m                 disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4834\u001b[0m                     \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4835\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    785\u001b[0m             )\n\u001b[1;32m    786\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 3827 has 14.74 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 255.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 3827 has 14.74 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 255.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"!pip install --upgrade trl transformers accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:08:04.969393Z","iopub.execute_input":"2025-05-18T18:08:04.969750Z","iopub.status.idle":"2025-05-18T18:08:19.029902Z","shell.execute_reply.started":"2025-05-18T18:08:04.969721Z","shell.execute_reply":"2025-05-18T18:08:19.029150Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.15.2)\nCollecting trl\n  Using cached trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nCollecting accelerate\n  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\nRequirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nUsing cached trl-0.17.0-py3-none-any.whl (348 kB)\nDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate, trl\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.5.2\n    Uninstalling accelerate-1.5.2:\n      Successfully uninstalled accelerate-1.5.2\n  Attempting uninstall: trl\n    Found existing installation: trl 0.15.2\n    Uninstalling trl-0.15.2:\n      Successfully uninstalled trl-0.15.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth-zoo 2025.5.7 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.17.0 which is incompatible.\nunsloth 2025.5.6 requires trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9, but you have trl 0.17.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.7.0 trl-0.17.0\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import os\nimport torch\nimport gc\nimport shutil\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    AutoModelForSequenceClassification\n)\nfrom peft import PeftModel\nfrom trl import PPOTrainer, PPOConfig\nfrom datasets import load_dataset\n\n# Set PyTorch memory configuration to reduce fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Check number of available GPUs\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of available GPUs: {num_gpus}\")\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# Check and display initial disk space\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Initial disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# Aggressive memory and disk cleanup\nprint(\"Performing aggressive cleanup before starting RLHF...\")\n# Delete all known references to previous objects\nglobal_vars_to_delete = [\n    'model', 'trainer', 'sft_dataset', 'val_dataset', 'sft_args',\n    'tokenized_train_rm_dataset', 'tokenized_val_rm_dataset',\n    'reward_model', 'reward_trainer'\n]\nfor var in global_vars_to_delete:\n    if var in globals():\n        print(f\"Deleting {var}...\")\n        del globals()[var]\n\n# Delete the base Reward Model to free space\nreward_base_model_dir = \"/kaggle/working/outputs/qwen_reward_base\"\nif os.path.exists(reward_base_model_dir):\n    shutil.rmtree(reward_base_model_dir)\n    print(f\"Deleted base Reward Model from {reward_base_model_dir}\")\n\n# Delete old checkpoints to free space\ncheckpoint_dir = \"/kaggle/working/outputs/checkpoints\"\nif os.path.exists(checkpoint_dir):\n    checkpoints = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n    if len(checkpoints) > 1:\n        for chk in checkpoints[:-1]:  # Keep the latest checkpoint\n            shutil.rmtree(os.path.join(checkpoint_dir, chk))\n            print(f\"Deleted checkpoint: {chk}\")\n\n# GPU memory cleanup\ntorch.cuda.reset_peak_memory_stats()\nfor _ in range(3):\n    torch.cuda.empty_cache()\n    gc.collect()\n    torch.cuda.synchronize()\n\n# Check memory and disk usage after cleanup\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated after cleanup: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage after cleanup - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\nprint(f\"\\n⚡️ Starting Reinforcement Learning Fine-Tuning (RLHF)\\n\")\n\n# Determine the device to load models on\ntarget_device = \"cuda:0\"\n\n# Load SFT-trained model with 4-bit quantization\nsft_model_save_path = \"/kaggle/working/outputs/sft_model\"\nprint(f\"Loading SFT-trained model in 4-bit from {sft_model_save_path}\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    sft_model_save_path,\n    load_in_4bit=True,\n    device_map=target_device,\n    attn_implementation=\"eager\"\n)\nif os.path.exists(os.path.join(sft_model_save_path, \"adapter_model.safetensors\")):\n    model = PeftModel.from_pretrained(model, sft_model_save_path, device_map=target_device)\nfor name, param in model.named_parameters():\n    if \"lora\" in name.lower():\n        param.data = param.data.to(torch.float16)\n        print(f\"Cast {name} to float16\")\ntokenizer = AutoTokenizer.from_pretrained(sft_model_save_path)\ntokenizer.padding_side = 'left'\nif tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\nmodel.config.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\nprint(f\"Model config - pad_token_id: {model.config.pad_token_id}, eos_token_id: {model.config.eos_token_id}\")\n\n# Load the trained Reward Model with 4-bit quantization\nreward_trained_model_dir = \"/kaggle/working/outputs/qwen_reward_trained\"\nprint(f\"Loading trained Reward Model in 4-bit from {reward_trained_model_dir}\")\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n    reward_trained_model_dir,\n    num_labels=1,\n    load_in_4bit=True,\n    device_map=target_device,\n    trust_remote_code=True\n)\nreward_tokenizer = AutoTokenizer.from_pretrained(reward_trained_model_dir, trust_remote_code=True)\n\n# --- START OF MANUAL REF_MODEL CREATION ---\nprint(\"Creating reference model on CPU...\")\n# Load the base model again specifically for the reference model, directly onto CPU\n# Ensure it's loaded in 4-bit for consistency if your main model is also 4-bit\nref_model = AutoModelForCausalLM.from_pretrained(\n    sft_model_save_path,\n    load_in_4bit=True, # Ensure consistency with main model if it's 4-bit\n    device_map=\"cpu\",  # <-- Load directly to CPU\n    attn_implementation=\"eager\"\n)\n# If the original SFT model was a PeftModel, the ref_model should also be a PeftModel\nif os.path.exists(os.path.join(sft_model_save_path, \"adapter_model.safetensors\")):\n    ref_model = PeftModel.from_pretrained(ref_model, sft_model_save_path, device_map=\"cpu\")\n# Ensure LoRA parameters are cast to float16 if they were trained as such\nfor name, param in ref_model.named_parameters():\n    if \"lora\" in name.lower():\n        param.data = param.data.to(torch.float16)\n\n# --- END OF MANUAL REF_MODEL CREATION ---\n\n\n# Memory and disk check after loading models\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated after loading models: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Disk usage after loading models - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# Mock train_rm_dataset if it's not defined\nif 'train_rm_dataset' not in globals():\n    print(\"train_rm_dataset not found, creating a mock dataset for demonstration.\")\n    from datasets import Dataset\n    dummy_data = {\n        \"problem\": [\n            \"What is 2+2?\",\n            \"Solve for x: x - 5 = 10\",\n            \"What is the capital of France?\",\n            \"Calculate the area of a rectangle with length 5 and width 3.\"\n        ]\n    }\n    train_rm_dataset = Dataset.from_dict(dummy_data)\n    print(f\"Mock train_rm_dataset created with {len(train_rm_dataset)} examples.\")\n\n# Prepare dataset for RLHF\nmax_seq_length = 64\ndef preprocess_rlhf_dataset(examples, tokenizer):\n    prompts = [f\"Problem: {p}\\nSolution: \" for p in examples[\"problem\"]]\n    tokenized_inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\")\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"]}\n\nrlhf_dataset = train_rm_dataset.map(\n    lambda x: preprocess_rlhf_dataset(x, tokenizer),\n    batched=True,\n    remove_columns=[col for col in train_rm_dataset.column_names if col not in ['input_ids', 'attention_mask']]\n)\n\n# Configure PPO for RLHF - NO ref_model_backend here\nppo_config = PPOConfig(\n    batch_size=1,\n    learning_rate=1e-5,\n    num_ppo_epochs=1,\n    mini_batch_size=1,\n    gradient_accumulation_steps=1,\n    #log_with=None # Remove this if not needed or causing issues\n    # ref_model_backend=\"cpu\", # <-- REMOVE THIS LINE\n)\n\n# Initialize PPO Trainer\nppo_trainer = PPOTrainer(\n    args=ppo_config,\n    model=model,\n    ref_model=ref_model, # <-- Pass the manually created CPU reference model\n    tokenizer=tokenizer,\n    train_dataset=rlhf_dataset,\n    reward_model=reward_model,\n    value_model=model,\n    data_collator=DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_seq_length)\n)\nprint(\"PPOTrainer initialized.\")\n\n# RLHF Training Loop\nprint(\"Starting RLHF training with PPO...\")\nfor step, batch in enumerate(ppo_trainer.dataloader):\n    # Ensure input tensors are on the same device as the model\n    query_tensors = batch[\"input_ids\"].to(target_device)\n    attention_mask = batch[\"attention_mask\"].to(target_device)\n\n    # Generate responses\n    response_tensors = ppo_trainer.model.policy.generate(\n        query_tensors,\n        attention_mask=attention_mask,\n        max_new_tokens=64,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        num_return_sequences=1\n    )\n\n    # Prepare inputs for the Reward Model\n    queries = [tokenizer.decode(q, skip_special_tokens=True) for q in query_tensors]\n    responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n    \n    # Extract the problem part from each query\n    reward_inputs = []\n    for q, r in zip(queries, responses):\n        problem_part = q.split('Problem: ')[1].split('\\nSolution: ')[0] if 'Problem: ' in q and '\\nSolution: ' in q else q\n        reward_input = f\"Problem: {problem_part}\\nSolution: {r}\"\n        reward_inputs.append(reward_input)\n\n    # Tokenize for reward model. Ensure inputs are on the same device as the reward model.\n    reward_tokenized = reward_tokenizer(reward_inputs, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\").to(target_device)\n\n    # Get rewards\n    with torch.no_grad():\n        reward_outputs = reward_model(**reward_tokenized)\n        rewards = reward_outputs.logits.squeeze()\n        if rewards.ndim == 0:\n            rewards = rewards.unsqueeze(0)\n\n    # PPO step\n    stats = ppo_trainer.step(\n        query_tensors,\n        response_tensors,\n        rewards\n    )\n\n    if step % 10 == 0:\n        print(f\"Step {step} - PPO stats: {stats}\")\n\n    if step >= 50:\n        print(f\"Reached step {step}, stopping training.\")\n        break\n\n# Save the RLHF-trained model\nrlhf_model_save_path = \"/kaggle/working/outputs/rlhf_model\"\nprint(f\"Saving RLHF-trained model to {rlhf_model_save_path}\")\nmodel.save_pretrained(rlhf_model_save_path, torch_dtype=torch.float16)\ntokenizer.save_pretrained(rlhf_model_save_path)\nprint(\"RLHF model saved. Contents:\", os.listdir(rlhf_model_save_path))\n\n# Clear GPU memory after training\nprint(\"Clearing GPU memory after RLHF training...\")\ntorch.cuda.empty_cache()\ndel model\ndel reward_model\ndel ref_model # Also delete the reference model\ndel ppo_trainer\ndel rlhf_dataset\ngc.collect()\nfor i in range(num_gpus):\n    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# Final disk usage check\ntotal, used, free = shutil.disk_usage(\"/kaggle/working\")\nprint(f\"Final disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:09:09.601845Z","iopub.execute_input":"2025-05-18T18:09:09.602472Z","iopub.status.idle":"2025-05-18T18:09:39.188399Z","shell.execute_reply.started":"2025-05-18T18:09:09.602441Z","shell.execute_reply":"2025-05-18T18:09:39.187338Z"}},"outputs":[{"name":"stdout","text":"Number of available GPUs: 2\nGPU 0 memory allocated: 8228.23 MB\nGPU 0 memory reserved: 11100.00 MB\nGPU 1 memory allocated: 18.38 MB\nGPU 1 memory reserved: 1558.00 MB\nInitial disk usage - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\nPerforming aggressive cleanup before starting RLHF...\nDeleting model...\nDeleting reward_model...\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"GPU 0 memory allocated after cleanup: 6021.14 MB\nGPU 0 memory reserved: 9288.00 MB\nGPU 1 memory allocated after cleanup: 18.38 MB\nGPU 1 memory reserved: 1558.00 MB\nDisk usage after cleanup - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\n\n⚡️ Starting Reinforcement Learning Fine-Tuning (RLHF)\n\nLoading SFT-trained model in 4-bit from /kaggle/working/outputs/sft_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937344850cb341aba523fe0f96f4b6fe"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"Model config - pad_token_id: 50257, eos_token_id: 50256\nLoading trained Reward Model in 4-bit from /kaggle/working/outputs/qwen_reward_trained\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b6a9c5293a4169813398f9c291b750"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"Creating reference model on CPU...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0003f5cc489742c19fbf65e5f6fe8092"}},"metadata":{}},{"name":"stdout","text":"GPU 0 memory allocated after loading models: 8208.37 MB\nGPU 0 memory reserved: 11056.00 MB\nGPU 1 memory allocated after loading models: 18.38 MB\nGPU 1 memory reserved: 1558.00 MB\nDisk usage after loading models - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/191 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a92d63088d24929b2b68b8f8599961b"}},"metadata":{}},{"name":"stdout","text":"PPOTrainer initialized.\nStarting RLHF training with PPO...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4257520255.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# PPO step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     stats = ppo_trainer.step(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mquery_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mresponse_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'UnslothPPOTrainer' object has no attribute 'step'"],"ename":"AttributeError","evalue":"'UnslothPPOTrainer' object has no attribute 'step'","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"# # %% Step 5: Reinforcement Learning Fine-Tuning (RLHF)\n# import os\n# import torch\n# import gc\n# import shutil\n# from transformers import (\n#     AutoModelForCausalLM,\n#     AutoTokenizer,\n#     DataCollatorWithPadding,\n#     AutoModelForSequenceClassification\n# )\n# from peft import PeftModel\n# from trl import PPOTrainer, PPOConfig\n# from datasets import load_dataset\n\n# # Set PyTorch memory configuration to reduce fragmentation\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# # Check number of available GPUs\n# num_gpus = torch.cuda.device_count()\n# print(f\"Number of available GPUs: {num_gpus}\")\n# for i in range(num_gpus):\n#     print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n#     print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# # Check and display initial disk space\n# total, used, free = shutil.disk_usage(\"/kaggle/working\")\n# print(f\"Initial disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# # Aggressive memory and disk cleanup\n# print(\"Performing aggressive cleanup before starting RLHF...\")\n# # Delete all known references to previous objects\n# global_vars_to_delete = [\n#     'model', 'trainer', 'sft_dataset', 'val_dataset', 'sft_args',\n#     'tokenized_train_rm_dataset', 'tokenized_val_rm_dataset',\n#     'reward_model', 'reward_trainer'\n# ]\n# for var in global_vars_to_delete:\n#     if var in globals():\n#         print(f\"Deleting {var}...\")\n#         del globals()[var]\n\n# # Delete the base Reward Model to free space\n# reward_base_model_dir = \"/kaggle/working/outputs/qwen_reward_base\"\n# if os.path.exists(reward_base_model_dir):\n#     shutil.rmtree(reward_base_model_dir)\n#     print(f\"Deleted base Reward Model from {reward_base_model_dir}\")\n\n# # Delete old checkpoints to free space\n# checkpoint_dir = \"/kaggle/working/outputs/checkpoints\"\n# if os.path.exists(checkpoint_dir):\n#     checkpoints = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d))]\n#     if len(checkpoints) > 1:\n#         for chk in checkpoints[:-1]:  # Keep the latest checkpoint\n#             shutil.rmtree(os.path.join(checkpoint_dir, chk))\n#             print(f\"Deleted checkpoint: {chk}\")\n\n# # GPU memory cleanup\n# torch.cuda.reset_peak_memory_stats()\n# for _ in range(3):\n#     torch.cuda.empty_cache()\n#     gc.collect()\n#     torch.cuda.synchronize()\n\n# # Check memory and disk usage after cleanup\n# for i in range(num_gpus):\n#     print(f\"GPU {i} memory allocated after cleanup: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n#     print(f\"GPU {i} memory reserved after cleanup: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n# total, used, free = shutil.disk_usage(\"/kaggle/working\")\n# print(f\"Disk usage after cleanup - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# print(f\"\\n⚡️ Starting Reinforcement Learning Fine-Tuning (RLHF)\\n\")\n\n# # Load SFT-trained model\n# sft_model_save_path = \"/kaggle/working/outputs/sft_model\"\n# print(f\"Loading SFT-trained model in float16 from {sft_model_save_path}\")\n# model = AutoModelForCausalLM.from_pretrained(\n#     sft_model_save_path,\n#     torch_dtype=torch.float16,\n#     device_map=\"auto\",  # Distribute across GPUs\n#     attn_implementation=\"eager\"\n# )\n# if os.path.exists(os.path.join(sft_model_save_path, \"adapter_model.safetensors\")):\n#     model = PeftModel.from_pretrained(model, sft_model_save_path, torch_dtype=torch.float16, device_map=\"auto\")\n# for name, param in model.named_parameters():\n#     if \"lora\" in name.lower():\n#         param.data = param.data.to(torch.float16)\n#         print(f\"Cast {name} to float16\")\n# tokenizer = AutoTokenizer.from_pretrained(sft_model_save_path)\n# tokenizer.padding_side = 'left'\n# if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:\n#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n#     model.resize_token_embeddings(len(tokenizer))\n# model.config.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n# model.config.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n# print(f\"Model config - pad_token_id: {model.config.pad_token_id}, eos_token_id: {model.config.eos_token_id}\")\n\n# # Load the trained Reward Model\n# reward_trained_model_dir = \"/kaggle/working/outputs/qwen_reward_trained\"\n# print(f\"Loading trained Reward Model from {reward_trained_model_dir}\")\n# reward_model = AutoModelForSequenceClassification.from_pretrained(\n#     reward_trained_model_dir,\n#     num_labels=1,\n#     torch_dtype=torch.float16,\n#     device_map=\"auto\",  # Distribute across GPUs\n#     trust_remote_code=True\n# )\n# reward_tokenizer = AutoTokenizer.from_pretrained(reward_trained_model_dir, trust_remote_code=True)\n\n# # Memory and disk check after loading models\n# for i in range(num_gpus):\n#     print(f\"GPU {i} memory allocated after loading models: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n#     print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n# total, used, free = shutil.disk_usage(\"/kaggle/working\")\n# print(f\"Disk usage after loading models - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")\n\n# # Prepare dataset for RLHF\n# max_seq_length = 64\n# def preprocess_rlhf_dataset(examples, tokenizer):\n#     prompts = [f\"Problem: {p}\\nSolution: \" for p in examples[\"problem\"]]\n#     tokenized_inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\")\n#     return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"]}\n\n# rlhf_dataset = train_rm_dataset.map(\n#     lambda x: preprocess_rlhf_dataset(x, tokenizer),\n#     batched=True,\n#     remove_columns=[col for col in train_rm_dataset.column_names if col not in ['input_ids', 'attention_mask']]\n# )\n\n# # Configure PPO for RLHF\n# ppo_config = PPOConfig(\n#     batch_size=1,\n#     learning_rate=1e-5,\n#     num_ppo_epochs=1,\n#     mini_batch_size=1,\n#     gradient_accumulation_steps=1,\n#     #log_with=None\n# )\n\n# # # Initialize PPO Trainer\n# # ppo_trainer = PPOTrainer(\n# #     config=ppo_config,\n# #     model=model,\n# #     ref_model=None,  # Use the SFT model as reference (no separate ref model needed)\n# #     tokenizer=tokenizer,\n# #     dataset=rlhf_dataset,\n# #     data_collator=DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_seq_length)\n# # )\n\n# # Initialize PPO Trainer\n# ppo_trainer = PPOTrainer(\n#     args=ppo_config,\n#     model=model,\n#     ref_model=None,         # Use the SFT model as reference\n#     tokenizer=tokenizer,\n#     train_dataset=rlhf_dataset,\n#     reward_model=reward_model,\n#     value_model=model,      # FIX: Pass the 'model' itself as the value model\n#     data_collator=DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_seq_length)\n# )\n# print(\"PPOTrainer initialized.\")\n\n# # RLHF Training Loop\n# print(\"Starting RLHF training with PPO...\")\n# for step, batch in enumerate(ppo_trainer.dataloader):\n#     query_tensors = batch[\"input_ids\"].to('cuda')\n#     attention_mask = batch[\"attention_mask\"].to('cuda')\n\n#     # Generate responses\n#     response_tensors = ppo_trainer.model.policy.generate( # Call generate on ppo_trainer.model.policy\n#         query_tensors,\n#         attention_mask=attention_mask,\n#         # REMOVED: max_length=max_seq_length,\n#         max_new_tokens=64, # FIX: Use max_new_tokens to specify how many NEW tokens to generate.\n#                            # Be aware: The total generated sequence length (input_length + max_new_tokens)\n#                            # might exceed the max_seq_length (64) used by reward_tokenizer,\n#                            # leading to truncation for reward calculation. Adjust this value\n#                            # based on expected response length and desired truncation behavior.\n#         do_sample=True,\n#         top_k=50,\n#         top_p=0.95,\n#         num_return_sequences=1\n#     )\n\n#     # Prepare inputs for the Reward Model\n#     queries = [tokenizer.decode(q, skip_special_tokens=True) for q in query_tensors]\n#     responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n    \n#     # Extract the problem part from each query\n#     reward_inputs = []\n#     for q, r in zip(queries, responses):\n#         # Split the query to isolate the problem part\n#         problem_part = q.split('Problem: ')[1].split('\\nSolution: ')[0] if 'Problem: ' in q and '\\nSolution: ' in q else q\n        \n#         # Construct the reward input in the required format\n#         reward_input = f\"Problem: {problem_part}\\nSolution: {r}\"\n#         reward_inputs.append(reward_input)\n\n#     # Tokenize for reward model. Note: max_length=max_seq_length here still applies,\n#     # so the combined 'Problem: ... Solution: ...' will be truncated to 64 tokens.\n#     reward_tokenized = reward_tokenizer(reward_inputs, truncation=True, padding=\"max_length\", max_length=max_seq_length, return_tensors=\"pt\").to('cuda')\n\n#     # Get rewards\n#     with torch.no_grad():\n#         reward_outputs = reward_model(**reward_tokenized)\n#         rewards = reward_outputs.logits.squeeze()\n#         if rewards.ndim == 0: # Ensure it's a 1-dim tensor if batch_size=1\n#             rewards = rewards.unsqueeze(0)\n\n#     # PPO step\n#     stats = ppo_trainer.step(\n#         query_tensors,\n#         response_tensors,\n#         rewards\n#     )\n\n#     if step % 10 == 0:\n#         print(f\"Step {step} - PPO stats: {stats}\")\n\n#     if step >= 50:  # Limit for demonstration; adjust as needed\n#         print(f\"Reached step {step}, stopping training.\")\n#         break\n\n# # Save the RLHF-trained model\n# rlhf_model_save_path = \"/kaggle/working/outputs/rlhf_model\"\n# print(f\"Saving RLHF-trained model to {rlhf_model_save_path}\")\n# model.save_pretrained(rlhf_model_save_path, torch_dtype=torch.float16)\n# tokenizer.save_pretrained(rlhf_model_save_path)\n# print(\"RLHF model saved. Contents:\", os.listdir(rlhf_model_save_path))\n\n# # Clear GPU memory after training\n# print(\"Clearing GPU memory after RLHF training...\")\n# torch.cuda.empty_cache()\n# del model\n# del reward_model\n# del ppo_trainer\n# del rlhf_dataset\n# gc.collect()\n# for i in range(num_gpus):\n#     print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n#     print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n\n# # Final disk usage check\n# total, used, free = shutil.disk_usage(\"/kaggle/working\")\n# print(f\"Final disk usage - Total: {total / (1024**3):.2f} GB, Used: {used / (1024**3):.2f} GB, Free: {free / (1024**3):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T18:01:10.420481Z","iopub.execute_input":"2025-05-18T18:01:10.421001Z","iopub.status.idle":"2025-05-18T18:01:20.530094Z","shell.execute_reply.started":"2025-05-18T18:01:10.420973Z","shell.execute_reply":"2025-05-18T18:01:20.528380Z"}},"outputs":[{"name":"stdout","text":"Number of available GPUs: 2\nGPU 0 memory allocated: 13520.06 MB\nGPU 0 memory reserved: 13784.00 MB\nGPU 1 memory allocated: 1603.61 MB\nGPU 1 memory reserved: 4712.00 MB\nInitial disk usage - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\nPerforming aggressive cleanup before starting RLHF...\nDeleting model...\nDeleting reward_model...\nGPU 0 memory allocated after cleanup: 6020.44 MB\nGPU 0 memory reserved after cleanup: 9244.00 MB\nGPU 1 memory allocated after cleanup: 18.38 MB\nGPU 1 memory reserved after cleanup: 1558.00 MB\nDisk usage after cleanup - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\n\n⚡️ Starting Reinforcement Learning Fine-Tuning (RLHF)\n\nLoading SFT-trained model in float16 from /kaggle/working/outputs/sft_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad3694e1aad466ca4ccadf9983ec4b7"}},"metadata":{}},{"name":"stdout","text":"Model config - pad_token_id: 50257, eos_token_id: 50256\nLoading trained Reward Model from /kaggle/working/outputs/qwen_reward_trained\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c37c855367045d69a8b5165288fc84d"}},"metadata":{}},{"name":"stdout","text":"GPU 0 memory allocated after loading models: 8917.66 MB\nGPU 0 memory reserved: 9276.00 MB\nGPU 1 memory allocated after loading models: 3086.35 MB\nGPU 1 memory reserved: 3138.00 MB\nDisk usage after loading models - Total: 19.52 GB, Used: 14.70 GB, Free: 4.80 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/191 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56e31091f42e43f9b1599227ac70d292"}},"metadata":{}},{"name":"stdout","text":"PPOTrainer initialized.\nStarting RLHF training with PPO...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/38657939.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Generate responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     response_tensors = ppo_trainer.model.policy.generate( # Call generate on ppo_trainer.model.policy\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mquery_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1063\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    920\u001b[0m                 )\n\u001b[1;32m    921\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m     ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n\u001b[1;32m    402\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         attn_outputs = self.attn(\n\u001b[1;32m    405\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/unsloth_compiled_cache/LayerNorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     return F.layer_norm(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     ).to(input.dtype)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m             return self._torchdynamo_orig_callable(\n\u001b[0m\u001b[1;32m   1433\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             return _compile(\n\u001b[0m\u001b[1;32m    599\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m             \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;31m# NB: We only put_code_state in success case.  Success case here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# in stack traces when profiling is not enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mStrobelightCompileTimeProfiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             return StrobelightCompileTimeProfiler.profile_compile_time(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_compile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         return (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mCompileContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestartAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mexit_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_function_mode_stack_state_mgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0mcleanup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mspeculation_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3500\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshould_compile_partial_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_graph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeculation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_generic_context_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2931\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mbreak_graph_if_unsupported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2932\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCOPY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;31m# if call_function fails, need to set kw_names to None, otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkw_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minner_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_forbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minline_generator_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/torch.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mfake_out_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"example_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m         tensor_variable = wrap_fx_proxy(\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             proxy=tx.output.create_proxy(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2300\u001b[0m     }\n\u001b[1;32m   2301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubclass_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_fx_proxy_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensorVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2303\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_fx_proxy_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensorWithTFOverrideVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2366\u001b[0m ):\n\u001b[1;32m   2367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexample_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2368\u001b[0;31m         return _wrap_fx_proxy(\n\u001b[0m\u001b[1;32m   2369\u001b[0m             \u001b[0mtarget_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36m_wrap_fx_proxy\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2462\u001b[0m         \u001b[0;31m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2463\u001b[0m         \u001b[0;31m# cases properly below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2464\u001b[0;31m         \u001b[0mexample_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fake_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_non_graph_fake\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2466\u001b[0m     return handle_traced_output(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   3227\u001b[0m             )\n\u001b[1;32m   3228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTorchRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_non_graph_fake\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   3125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_python_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3127\u001b[0;31m             ret_val = wrap_fake_exception(\n\u001b[0m\u001b[1;32m   3128\u001b[0m                 \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3129\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   2639\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrap_fake_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2640\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2641\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2642\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupportedFakeTensorException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2643\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munimplemented_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_python_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3127\u001b[0m             ret_val = wrap_fake_exception(\n\u001b[0;32m-> 3128\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3129\u001b[0m             )\n\u001b[1;32m   3130\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   3323\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3324\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3325\u001b[0;31m             raise RuntimeError(make_error_message(e)).with_traceback(\n\u001b[0m\u001b[1;32m   3326\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3327\u001b[0m             ) from e\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   3282\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_function\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3284\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_method\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3286\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_stats.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         ), func\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fake tensor raised TypeError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_UNASSIGNED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2331\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m                     return maybe_propagate_real_tensors(\n\u001b[0;32m-> 2333\u001b[0;31m                         \u001b[0mdecomposition_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2334\u001b[0m                     )\n\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mis_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mis_none\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_none\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mout_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         out_params = [\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             assert (\n\u001b[1;32m    310\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36mnative_layer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3283\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_convert_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_stats.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         ), func\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fake tensor raised TypeError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_UNASSIGNED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2311\u001b[0m             \u001b[0mfast_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fast_op_impls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfast_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_propagate_real_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;31m# If there's a Python meta, prefer that over the decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_impls.py\u001b[0m in \u001b[0;36mfast_binary_impl\u001b[0;34m(mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0mcurrent_cpu_scalars_on_non_cpu\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcommon_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mslow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;31m# compute_fast_setup_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_impls.py\u001b[0m in \u001b[0;36mslow\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mcount_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"slow {msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mslow_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mcount_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attempt fast\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             assert (\n\u001b[1;32m    310\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims_common/wrappers.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpromoted_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Override the return_dtype if a dtype arg is present and not None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m_ref\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             )\n\u001b[1;32m   1085\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_noncontiguous_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1691\u001b[0m )\n\u001b[1;32m   1692\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorLikeType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorLikeType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensorLikeType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_stats.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         ), func\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fake tensor raised TypeError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_UNASSIGNED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 return maybe_propagate_real_tensors(\n\u001b[0;32m-> 2355\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprim_meta_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m                 )\n\u001b[1;32m   2357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims/__init__.py\u001b[0m in \u001b[0;36m_prim_elementwise_meta\u001b[0;34m(type_promotion, args_with_fixed_dtypes, *args)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0margs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_with_fixed_dtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_same_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_cpu_scalar_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_same_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_cpu_scalar_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_prims_common/__init__.py\u001b[0m in \u001b[0;36mcheck_same_device\u001b[0;34m(allow_cpu_scalar_tensors, *args)\u001b[0m\n\u001b[1;32m    777\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0;34m\"!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                 )\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             msg = (\n","\u001b[0;31mTorchRuntimeError\u001b[0m: Dynamo failed to run FX node with fake tensors: call_function <built-in method layer_norm of type object at 0x7b0379af6f80>(*(FakeTensor(..., device='cuda:1', size=(s0, s1, 1600), dtype=torch.float16), (1600,), Parameter(FakeTensor(..., device='cuda:0', size=(1600,), dtype=torch.float16,\n           requires_grad=True)), Parameter(FakeTensor(..., device='cuda:0', size=(1600,), dtype=torch.float16,\n           requires_grad=True)), 1e-05, True), **{}): got RuntimeError('Tensor on device cuda:0 is not on the expected device cuda:1!')\n\nfrom user code:\n   File \"/usr/local/lib/python3.11/dist-packages/unsloth_zoo/patch_torch_functions.py\", line 67, in layer_norm\n    return torch.layer_norm(\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"],"ename":"TorchRuntimeError","evalue":"Dynamo failed to run FX node with fake tensors: call_function <built-in method layer_norm of type object at 0x7b0379af6f80>(*(FakeTensor(..., device='cuda:1', size=(s0, s1, 1600), dtype=torch.float16), (1600,), Parameter(FakeTensor(..., device='cuda:0', size=(1600,), dtype=torch.float16,\n           requires_grad=True)), Parameter(FakeTensor(..., device='cuda:0', size=(1600,), dtype=torch.float16,\n           requires_grad=True)), 1e-05, True), **{}): got RuntimeError('Tensor on device cuda:0 is not on the expected device cuda:1!')\n\nfrom user code:\n   File \"/usr/local/lib/python3.11/dist-packages/unsloth_zoo/patch_torch_functions.py\", line 67, in layer_norm\n    return torch.layer_norm(\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"# Load reward model\nprint(f\"Loading reward model from {reward_trained_model_dir}\")\ntrained_reward_model = AutoModelForSequenceClassification.from_pretrained(\n    reward_trained_model_dir,\n    num_labels=1,\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map=None\n)\ntrained_reward_model = trained_reward_model.to('cuda:0')\nif os.path.exists(os.path.join(reward_trained_model_dir, \"adapter_model.safetensors\")):\n    trained_reward_model = PeftModelForSequenceClassification.from_pretrained(\n        trained_reward_model,\n        reward_trained_model_dir,\n        torch_dtype=torch.float16,\n        device_map=None\n    )\n    trained_reward_model = trained_reward_model.to('cuda:0')\nfor name, param in trained_reward_model.named_parameters():\n    if \"lora\" in name.lower():\n        param.data = param.data.to(torch.float16)\n        print(f\"Cast {name} to float16\")\ntrained_reward_tokenizer = AutoTokenizer.from_pretrained(reward_trained_model_dir, trust_remote_code=True)\ntrained_reward_tokenizer.padding_side = 'left'\nif trained_reward_tokenizer.pad_token is None or trained_reward_tokenizer.pad_token == trained_reward_tokenizer.eos_token:\n    trained_reward_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntrained_reward_model.config.pad_token_id = trained_reward_tokenizer.convert_tokens_to_ids(trained_reward_tokenizer.pad_token)\nprint(f\"Reward model pad_token_id: {trained_reward_model.config.pad_token_id}\")\n\nprint(f\"Policy model device: {next(model.parameters()).device}\")\nprint(f\"Reward model device: {next(trained_reward_model.parameters()).device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_reward_score(problem_texts, response_texts, reward_model, reward_tokenizer, device):\n    reward_model.eval()\n    reward_model = reward_model.to(device)\n    texts_for_rm = [f\"Problem: {p}\\nSolution: {r}\" for p, r in zip(problem_texts, response_texts)]\n    inputs = reward_tokenizer(\n        texts_for_rm,\n        padding=True,\n        truncation=True,\n        max_length=max_seq_length,\n        return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = reward_model(**inputs)\n        scores = outputs.logits.squeeze(-1)\n    return scores.cpu()\n\nppo_args = TrainingArguments(\n    output_dir=os.path.join(output_dir, \"ppo_checkpoints\"),\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    learning_rate=1e-5,\n    num_train_epochs=4,\n    logging_steps=10,\n    save_strategy=\"steps\",\n    save_steps=50,\n    report_to=\"none\",\n    fp16=True,\n    bf16=False\n)\n\nppo_config = PPOConfig(\n    mini_batch_size=2,\n    batch_size=8\n)\n\nmax_ppo_epochs = 4\n\nprint(f\"Loading reference model from {sft_model_save_path}\")\nref_model = AutoModelForCausalLM.from_pretrained(\n    sft_model_save_path,\n    torch_dtype=torch.float16,\n    device_map=None,\n    attn_implementation=\"eager\"\n)\nref_model = ref_model.to('cuda:0')\nif os.path.exists(os.path.join(sft_model_save_path, \"adapter_model.safetensors\")):\n    ref_model = PeftModel.from_pretrained(ref_model, sft_model_save_path, torch_dtype=torch.float16, device_map=None)\n    ref_model = ref_model.to('cuda:0')\nfor name, param in ref_model.named_parameters():\n    if \"lora\" in name.lower():\n        param.data = param.data.to(torch.float16)\n        print(f\"Cast {name} to float16\")\n    param.requires_grad = False\nref_model.eval()\n\nprint(f\"Loading value model from {sft_model_save_path}\")\nvalue_model = AutoModelForCausalLM.from_pretrained(\n    sft_model_save_path,\n    torch_dtype=torch.float16,\n    device_map=None,\n    attn_implementation=\"eager\"\n)\nvalue_model = value_model.to('cuda:0')\nif os.path.exists(os.path.join(sft_model_save_path, \"adapter_model.safetensors\")):\n    value_model = PeftModel.from_pretrained(value_model, sft_model_save_path, torch_dtype=torch.float16, device_map=None)\n    value_model = value_model.to('cuda:0')\nfor name, param in value_model.named_parameters():\n    if \"lora\" in name.lower():\n        param.data = param.data.to(torch.float16)\n        print(f\"Cast {name} to float16\")\n\nprint(f\"PPOTrainer device: {ppo_trainer.accelerator.device}\")\nprint(f\"Policy model device: {next(model.parameters()).device}\")\nprint(f\"Reference model device: {next(ref_model.parameters()).device}\")\nprint(f\"Value model device: {next(value_model.parameters()).device}\")\nprint(f\"Reward model device: {next(trained_reward_model.parameters()).device}\")\n\ndef tokenize_rlhf_dataset_for_ppo(examples, tokenizer):\n    tokenized_inputs = tokenizer(\n        examples[\"problem\"],\n        truncation=True,\n        max_length=max_seq_length,\n        padding='max_length',\n        return_attention_mask=True\n    )\n    tokenized_inputs[\"problem\"] = examples[\"problem\"]\n    return tokenized_inputs\n\ntokenized_rlhf_dataset = rlhf_dataset.map(\n    lambda x: tokenize_rlhf_dataset_for_ppo(x, tokenizer),\n    batched=True,\n    remove_columns=[col for col in rlhf_dataset.column_names if col not in ['input_ids', 'attention_mask', 'problem']]\n)\nprint(\"✅ RLHF dataset tokenized for PPO.\")\n\nprint(\"Setting generation parameters on model.generation_config...\")\nmodel.generation_config.max_new_tokens = max_seq_length\nmodel.generation_config.eos_token_id = tokenizer.eos_token_id\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\nmodel.generation_config.do_sample = True\nmodel.generation_config.top_k = 50\nmodel.generation_config.top_p = 0.95\nprint(\"Generation parameters configured.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PPODatasetCollator(DataCollatorWithPadding):\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n        batch = {}\n        tensor_features = [{k: v for k, v in feature.items() if k in ['input_ids', 'attention_mask']} for feature in features]\n        padded_inputs = super().__call__(tensor_features)\n        batch.update({\n            'input_ids': padded_inputs['input_ids'].to(torch.int64),\n            'attention_mask': padded_inputs['attention_mask'].to(torch.int64)\n        })\n        batch['problem'] = [feature['problem'] for feature in features]\n        return batch\n\nppo_trainer = PPOTrainer(\n    model=model,\n    ref_model=ref_model,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_rlhf_dataset,\n    args=ppo_args,\n    reward_model=trained_reward_model,\n    value_model=value_model,\n    data_collator=PPODatasetCollator(tokenizer=tokenizer, padding=True, max_length=max_seq_length)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting RLHF training with manual loop...\")\nrlhf_accuracies = []\nrlhf_reasonings = []\n\nfor epoch in range(max_ppo_epochs):\n    print(f\"\\n--- RLHF Epoch {epoch + 1}/{max_ppo_epochs} ---\")\n    for batch in ppo_trainer.dataloader:\n        query_tensors = batch[\"input_ids\"].to(ppo_trainer.accelerator.device, dtype=torch.int64)\n        attention_mask = batch[\"attention_mask\"].to(ppo_trainer.accelerator.device, dtype=torch.int64)\n        problem_texts_batch = batch[\"problem\"]\n\n        print(f\"query_tensors shape: {query_tensors.shape}, dtype: {query_tensors.dtype}\")\n        print(f\"attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}\")\n        print(f\"attention_mask sample: {attention_mask[0]}\")\n        assert torch.all((attention_mask == 0) | (attention_mask == 1)), \"Attention mask contains invalid values\"\n\n        response_tensors = model.generate(\n            input_ids=query_tensors,\n            attention_mask=attention_mask,\n            max_new_tokens=max_seq_length,\n            do_sample=True,\n            top_p=0.95,\n            top_k=50,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n        response_texts = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n\n        rewards = get_reward_score(problem_texts_batch, response_texts, trained_reward_model, trained_reward_tokenizer, device=ppo_trainer.accelerator.device)\n        rewards = rewards.detach()\n\n        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n\n    print(f\"\\nEvaluating model after RLHF Epoch {epoch + 1}...\")\n    epoch_accuracy, epoch_reasoning = evaluate_model(reward_val, model, tokenizer)\n    rlhf_accuracies.append(epoch_accuracy)\n    rlhf_reasonings.append(epoch_reasoning)\n    print(f\"Epoch {epoch + 1} - Validation Accuracy: {epoch_accuracy:.2%}, Reasoning Quality: {epoch_reasoning:.2f}\")\n\nprint(\"\\n📊 Evaluating Final RLHF Model...\")\ntry:\n    final_accuracy, final_reasoning = evaluate_model(reward_val, model, tokenizer)\n    print(f\"\\n✨ Final Evaluation - Accuracy: {final_accuracy:.2%}, Reasoning Quality: {final_reasoning:.2f}\\n\")\nexcept Exception as e:\n    print(f\"Error during final evaluation: {str(e)}\")\n    raise\n\nprint(f\"💾 Saving final PPO-trained policy model to {ppo_model_save_path}\")\ntry:\n    model.save_pretrained(ppo_model_save_path)\n    tokenizer.save_pretrained(ppo_model_save_path)\n    print(\"✅ Final PPO-trained policy model saved.\")\nexcept Exception as e:\n    print(f\"Error saving final PPO model: {str(e)}\")\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, max_ppo_epochs + 1), rlhf_accuracies, marker='o', label=\"RLHF Validation Accuracy\")\nplt.plot(range(1, max_ppo_epochs + 1), rlhf_reasonings, marker='x', label=\"RLHF Validation Reasoning Quality\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.title(\"RLHF Validation Metrics per Epoch\")\nplt.legend()\nplt.savefig(os.path.join(output_dir, \"rlhf_validation_metrics.png\"))\nplt.close()\nprint(f\"RLHF validation metrics plot saved to {os.path.join(output_dir, 'rlhf_validation_metrics.png')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}